---
phase: 05-investigation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/utils/knowledgeGating.ts
  - src/utils/promptTemplates.ts
  - src/utils/innerVoiceTemplates.ts
  - api/dialogue.ts
autonomous: true

must_haves:
  truths:
    - "NPCs only reveal information appropriate to current trust level"
    - "NPC responses appear progressively (streaming) for natural pacing"
    - "NPC dialogue matches DitV frontier religious period voice"
    - "Inner voice reflects character's highest stat during conversations"
  artifacts:
    - path: "src/utils/knowledgeGating.ts"
      provides: "Trust-level and approach filtering of NPC knowledge"
      exports: ["filterKnowledgeByTrust", "getAvailableTopics"]
    - path: "src/utils/promptTemplates.ts"
      provides: "LLM system and user prompt construction"
      exports: ["buildSystemPrompt", "buildUserPrompt"]
    - path: "src/utils/innerVoiceTemplates.ts"
      provides: "Stat-based inner voice interjection templates"
      exports: ["getInnerVoiceInterjection", "INNER_VOICE_TEMPLATES"]
    - path: "api/dialogue.ts"
      provides: "Serverless function proxying LLM calls with streaming"
      exports: ["POST"]
  key_links:
    - from: "api/dialogue.ts"
      to: "src/utils/knowledgeGating.ts"
      via: "Server-side knowledge filtering before prompt construction"
      pattern: "filterKnowledgeByTrust"
    - from: "api/dialogue.ts"
      to: "src/utils/promptTemplates.ts"
      via: "Prompt construction from filtered knowledge"
      pattern: "buildSystemPrompt.*buildUserPrompt"
---

<objective>
Build the LLM integration layer: knowledge gating utilities, prompt templates, inner voice templates, and the serverless API endpoint.

Purpose: The dialogue system's integrity depends on server-side knowledge filtering (trust gates prevent NPCs from revealing information they shouldn't) and well-crafted prompts (DitV period voice, brevity constraints). This plan creates the backend pipeline that the dialogue UI will consume.

Output: Working serverless function that accepts conversation requests, filters NPC knowledge by trust level and approach, constructs guarded prompts, and streams LLM responses. Plus template-based inner voice system.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05-investigation/05-CONTEXT.md
@.planning/phases/05-investigation/05-RESEARCH.md
@src/types/dialogue.ts
@src/types/investigation.ts
@src/types/npc.ts
@src/types/character.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Knowledge gating and prompt template utilities</name>
  <files>src/utils/knowledgeGating.ts, src/utils/promptTemplates.ts, src/utils/innerVoiceTemplates.ts</files>
  <action>
Create `src/utils/knowledgeGating.ts`:
- `filterKnowledgeByTrust(facts: KnowledgeFact[], trustLevel: number, approach?: ApproachType): KnowledgeFact[]`
  - Filters facts where trustLevel >= fact.minTrustLevel
  - Additionally filters by fact.requiredApproach if set (only pass if approach matches)
  - Returns filtered subset (what NPC CAN reveal this conversation)
- `getAvailableTopics(npcKnowledge: NPCKnowledge, discoveries: Discovery[], currentLocation: string): Topic[]`
  - Always-available topics: "greeting", "town" (generic)
  - Discovery-unlocked topics: check if discovery.requiresDiscovery ID exists in player's discoveries
  - Location-specific topics: only if currentLocation matches topic.locationOnly
  - Returns Topic[] with available flags set

Create `src/utils/promptTemplates.ts`:
- `buildSystemPrompt(npcName: string, npcRole: string, personality: string, filteredFacts: KnowledgeFact[]): string`
  - Template: "You are {name}, {role} in this frontier town of the Faith."
  - PERSONALITY section from NPC data
  - SPEECH PATTERN: "Address the player as 'Brother' or 'Sister'. Use biblical cadence, frontier religious community language of the 1850s. Be concise."
  - KNOWLEDGE YOU POSSESS: bullet list of filtered fact contents
  - CRITICAL CONSTRAINTS: cannot reveal unlisted info, deflect unknown topics in character, keep responses under 60 words, respond as both the player's spoken words AND the NPC's response (format: "[Player]: ...\n[NPC]: ...")
- `buildUserPrompt(topic: string, approach: ApproachType, statValue: number): string`
  - Describes the player's chosen topic and approach style
  - statValue (2-6) influences how effective/confident the approach sounds
  - Acuity: observing/deducing. Heart: empathizing/comforting. Body: intimidating/pressuring. Will: commanding/preaching.
  - Higher stat = more direct and effective approach description

Create `src/utils/innerVoiceTemplates.ts`:
- `INNER_VOICE_TEMPLATES` object keyed by StatName, each with templates for situations: 'npc-evasion', 'new-discovery', 'sin-connection', 'conflict-risk', 'trust-low', 'trust-high'
- 3-4 template strings per situation per stat (randomly selected for variety)
- Acuity notices inconsistencies, logical gaps, hidden meanings
- Heart reads emotional pain, fear, love, desperation
- Body sizes up physical threats, readiness, weakness
- Will senses moral weight, duty, spiritual corruption
- `getInnerVoiceInterjection(stat: StatName, situation: string): string | null`
  - 30% chance to trigger (return null 70% of time)
  - Returns random template from matching stat+situation
  - Returns null if no templates match situation

All templates should use DitV period voice and frontier religious setting.
  </action>
  <verify>Run `npx tsc --noEmit` - no type errors. Verify filterKnowledgeByTrust correctly filters a test set of facts (add inline type assertions if needed).</verify>
  <done>Knowledge gating filters facts by trust+approach, prompts enforce DitV voice and 60-word brevity, inner voice has 4 stats x 6 situations x 3-4 templates.</done>
</task>

<task type="auto">
  <name>Task 2: Serverless dialogue API endpoint</name>
  <files>api/dialogue.ts, package.json</files>
  <action>
Install Vercel AI SDK and Anthropic provider:
```bash
npm install ai @ai-sdk/anthropic
```

Create `api/dialogue.ts` (Vercel serverless function):
- Import { streamText } from 'ai' and { anthropic } from '@ai-sdk/anthropic'
- Import knowledgeGating and promptTemplates utilities
- Export async POST(request: Request) handler:
  1. Parse request body: { npcId, npcName, npcRole, npcPersonality, npcFacts: KnowledgeFact[], topic, approach, trustLevel, statValue }
  2. Call filterKnowledgeByTrust(npcFacts, trustLevel, approach) to get safe facts
  3. Call buildSystemPrompt(npcName, npcRole, npcPersonality, safeFacts)
  4. Call buildUserPrompt(topic, approach, statValue)
  5. Call streamText with anthropic('claude-sonnet-4-20250514'), temperature 0.7, maxTokens 300
  6. Return result.toTextStreamResponse()
- Add error handling: try/catch wrapping LLM call, return 500 with { error: message } on failure
- Add input validation: verify approach is valid ApproachType enum value, trustLevel is number
- Set maxDuration in function config if Vercel supports it (prevent timeout)

Note: The API endpoint receives NPC data from the client (since town data lives client-side in this phase). In Phase 6, this may shift to server-side NPC lookup. The trust-gating still happens server-side to prevent prompt injection.

For local development without Vercel, also create a simple express-like handler pattern that the Vite dev server can proxy to. Add a vite.config.ts proxy entry: '/api/dialogue' -> local handler (or mock for dev).

Create a mock handler for development that returns canned responses without calling LLM:
- src/utils/mockDialogue.ts: mockDialogueResponse(topic, approach, npcName) returns a streaming-compatible response with pre-written DitV-style dialogue for common topic/approach combos
- This mock is used in dev mode (import.meta.env.DEV) so development doesn't require API keys
  </action>
  <verify>Run `npx tsc --noEmit` - no type errors. Run `npm run build` to verify no import issues. Verify mock handler returns string response for dev mode.</verify>
  <done>Serverless endpoint proxies LLM with trust-gated knowledge, mock handler enables local dev without API keys, AI SDK installed and configured.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes
- `npm run build` succeeds (no import resolution failures)
- ai and @ai-sdk/anthropic in package.json dependencies
- filterKnowledgeByTrust correctly excludes facts above trust threshold
- buildSystemPrompt includes CRITICAL CONSTRAINTS section
- Mock dialogue handler returns DitV-style text without API calls
</verification>

<success_criteria>
- Knowledge filtering prevents high-trust facts from leaking to low-trust conversations
- System prompt enforces 60-word limit and DitV period voice
- API endpoint streams responses (not blocking)
- Dev mode works without API keys via mock handler
- Inner voice templates cover all 4 stats with contextual variety
</success_criteria>

<output>
After completion, create `.planning/phases/05-investigation/05-02-SUMMARY.md`
</output>
